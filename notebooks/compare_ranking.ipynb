{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from botorch.utils.multi_objective import pareto\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting up the MRL scaler  \n",
    "The Optimus5Prime provided in the directory is trained using a normalized MRL values, and therefore predict normalized values. The scaler loaded here will transform the noramlized values back to the actual MRL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = pd.read_csv(os.path.join(\"./data/GSM3130435_egfp_unmod_1_processed.tsv\"), sep=\"\\t\")[\"rl\"]\n",
    "scaler = preprocessing.StandardScaler().fit(np.array(label_data).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_obj_to_real(df_tmp, column_name, obj_name):\n",
    "    if obj_name == \"AGC content [%]\":\n",
    "        obj_val = np.array(df_tmp[column_name])\n",
    "        seq_length = len(df_tmp[\"cand_seq\"].iloc[0])\n",
    "        obj_val = (seq_length - obj_val) / seq_length * 100\n",
    "    elif obj_name == \"MRL\":\n",
    "        obj_val = np.array(df_tmp[column_name])\n",
    "        obj_val = scaler.inverse_transform(-obj_val.reshape(-1,1)).flatten()\n",
    "    elif obj_name == \"G4 score\":\n",
    "        obj_val = - np.array(df_tmp[column_name])\n",
    "    elif obj_name == \"in vitro stability\":\n",
    "        obj_val = - np.array(df_tmp[column_name])\n",
    "    return obj_val\n",
    "\n",
    "def format_to_rna(tmp_array):\n",
    "    for i, seq in enumerate(tmp_array):\n",
    "        tmp_array[i] = seq.replace(\"T\", \"U\")\n",
    "    return tmp_array\n",
    "    \n",
    "def format_init(df_init):\n",
    "    df_init = df_init.rename(\n",
    "        columns={\n",
    "            \"sequence\":\"cand_seq\",\n",
    "            \"nonU_cotent\":\"obj_val_0\",\n",
    "            \"mrl\":\"obj_val_1\",\n",
    "            \"G4\":\"obj_val_2\",\n",
    "            \"degradation\":\"obj_val_3\"\n",
    "        }\n",
    "    )\n",
    "    df_init[\"round_idx\"] = [0,]*len(df_init)\n",
    "    df_init[\"cand_uuid\"] = [\"\",]*len(df_init)\n",
    "    df_init[\"cand_ancestor\"] = [\"\",]*len(df_init)\n",
    "    # df_init = df_init.reindex(columns=[\"round_idx\",\"cand_uuid\",\"cand_ancestor\",\"cand_seq\",\"obj_val_0\",\"obj_val_1\",\"obj_val_2\",\"obj_val_3\"])\n",
    "    return df_init\n",
    "\n",
    "# assume all objectives are to be maximized\n",
    "def pareto_ranking(df_all, df_pareto, obj_names):\n",
    "    rank_scores = []\n",
    "    for i in range(len(df_pareto)):\n",
    "        df_tmp = df_all.copy()\n",
    "        for tmp_obj in obj_names:\n",
    "            df_tmp = df_tmp[df_tmp[tmp_obj]<=df_pareto[tmp_obj].iloc[i]]\n",
    "        rank_scores.append(len(df_tmp)+1)\n",
    "    df_pareto[\"score_pareto_ranking\"] = rank_scores\n",
    "    df_pareto = df_pareto.sort_values(by=\"score_pareto_ranking\", ascending=False)\n",
    "    return df_pareto\n",
    "\n",
    "# assume all objectives are to be maximized\n",
    "def pareto_R_method(df_pareto, obj_names, obj_rank_values):\n",
    "    # calculate rank weights for objectives\n",
    "    obj_rank = pd.DataFrame(obj_rank_values, columns=[\"rank\"])\n",
    "    obj_rank[\"rank_inv\"] = 1/obj_rank[\"rank\"]\n",
    "\n",
    "    tmp_weights = []\n",
    "    obj_rank = obj_rank.sort_values(by=\"rank\")\n",
    "    tmp_values = np.sort(np.unique(obj_rank[\"rank\"]))\n",
    "    for tmp_value in tmp_values:\n",
    "        tmp_rows = obj_rank[obj_rank[\"rank\"]==tmp_value]\n",
    "        tmp_weight = 1/np.sum(obj_rank[obj_rank[\"rank\"]<=tmp_value][\"rank_inv\"])\n",
    "        tmp_weights.extend([tmp_weight]*len(tmp_rows))\n",
    "\n",
    "    obj_rank[\"tmp_weight\"] = tmp_weights\n",
    "    obj_rank[\"rank_weight\"] = obj_rank[\"tmp_weight\"] / np.sum(obj_rank[\"tmp_weight\"])\n",
    "    obj_rank_weights = obj_rank[\"rank_weight\"].values\n",
    "\n",
    "    # calculate rank weights for pareto points\n",
    "    df_tmp = df_pareto.copy()\n",
    "    for tmp_obj in obj_names:\n",
    "        df_tmp = df_tmp.sort_values(by=tmp_obj, ascending=False)\n",
    "        df_tmp[\"rank_{}\".format(tmp_obj)] = np.arange(1,len(df_tmp)+1)\n",
    "        tmp_values = np.sort(np.unique(df_tmp[tmp_obj]))[::-1]\n",
    "        if len(tmp_values)==len(df_tmp):\n",
    "            pass\n",
    "        else:\n",
    "            new_rank = []\n",
    "            for tmp_value in tmp_values:\n",
    "                tmp_rows = df_tmp[df_tmp[tmp_obj]==tmp_value]\n",
    "                if len(tmp_rows)>1:\n",
    "                    new_rank_value = np.sum(tmp_rows[\"rank_{}\".format(tmp_obj)].values)/len(tmp_rows)\n",
    "                    new_rank.extend([new_rank_value]*len(tmp_rows))\n",
    "                else:\n",
    "                    new_rank.append(tmp_rows[\"rank_{}\".format(tmp_obj)].values[0])\n",
    "            df_tmp[\"rank_{}\".format(tmp_obj)] = new_rank\n",
    "        df_tmp[\"rank_inv_{}\".format(tmp_obj)] = 1/df_tmp[\"rank_{}\".format(tmp_obj)]\n",
    "\n",
    "        tmp_weights = []\n",
    "        tmp_values = np.sort(np.unique(df_tmp[\"rank_{}\".format(tmp_obj)]))\n",
    "        for tmp_value in tmp_values:\n",
    "            tmp_rows = df_tmp[df_tmp[\"rank_{}\".format(tmp_obj)]==tmp_value]\n",
    "            tmp_weight = 1/np.sum(df_tmp[df_tmp[\"rank_{}\".format(tmp_obj)]<=tmp_value][\"rank_inv_{}\".format(tmp_obj)])\n",
    "            tmp_weights.extend([tmp_weight]*len(tmp_rows))\n",
    "        df_tmp[\"tmp_weight\"] = tmp_weights\n",
    "        df_pareto[\"rank_weight_{}\".format(tmp_obj)] = df_tmp[\"tmp_weight\"] / np.sum(df_tmp[\"tmp_weight\"])\n",
    "\n",
    "    df_pareto[\"score_Rmethod\"] = np.zeros(len(df_pareto))\n",
    "    for i, tmp_obj in enumerate(obj_names):\n",
    "        df_pareto[\"score_Rmethod\"] += df_pareto[\"rank_weight_{}\".format(tmp_obj)] * obj_rank_weights[i]\n",
    "    df_pareto = df_pareto.sort_values(by=\"score_Rmethod\", ascending=False)\n",
    "    return df_pareto\n",
    "\n",
    "from itertools import product\n",
    "def generate_MIPS_weights(N, w):\n",
    "    max_multiples = int(1 / w)\n",
    "    possible_values = [i * w for i in range(max_multiples + 1)]\n",
    "    all_combinations = product(possible_values, repeat=N)\n",
    "    \n",
    "    # Filter combinations where the sum of elements is exactly 1\n",
    "    valid_combinations = [comb for comb in all_combinations if np.isclose(sum(comb), 1)]\n",
    "    result_array = np.array(valid_combinations)\n",
    "    return result_array\n",
    "\n",
    "# assume all objectives are to be maximized\n",
    "def pareto_MIPS(df_pareto, obj_names, weights=[]):\n",
    "    # parameter in augmented weighted Tchebycheff\n",
    "    rho = 0.0\n",
    "    # window of weights\n",
    "    wind = 0.01\n",
    "    # value of Tstar\n",
    "    Tstar = 0.01\n",
    "\n",
    "    obj_values = - np.array(df_pareto[obj_names])\n",
    "    # min-max normalization\n",
    "    obj_norm = (obj_values - np.min(obj_values, axis=0))/(np.max(obj_values, axis=0) - np.min(obj_values, axis=0))\n",
    "    num_samples = obj_norm.shape[0]\n",
    "\n",
    "    if len(weights)==0:\n",
    "        weights = generate_weights(len(obj_names), w=wind)\n",
    "\n",
    "    FT_list=[]\n",
    "    pareto_list = []\n",
    "    # H_all = []\n",
    "    for weight in weights:\n",
    "        H = np.max(np.multiply(np.tile(np.array(weight), obj_norm.shape[0]).reshape(-1,len(weight)), obj_norm), axis=1)\n",
    "        H += rho * np.sum(obj_norm, axis=1)\n",
    "        \n",
    "        Emin, Emax = np.min(H), np.max(H)\n",
    "        pareto_list.append(np.argmin(H))\n",
    "        # H_all.append((H-Emin)/(Emax-Emin))\n",
    "        \n",
    "        H_norm = (H-Emin)/(Emax-Emin)\n",
    "        Z = 0.0\n",
    "        for i in range(num_samples):\n",
    "            Z += np.exp(-H_norm[i]/Tstar)\n",
    "\n",
    "        if Z==0.0:\n",
    "            FT_list.append(0.0)\n",
    "        else:\n",
    "            FT_list.append(- Tstar*(np.log(Z)))\n",
    "\n",
    "    arg_index = np.array(FT_list).argsort()[::-1]\n",
    "    ranking_index = []\n",
    "    ranking_MIPS = []\n",
    "\n",
    "    for i in range(len(arg_index)):\n",
    "        if pareto_list[arg_index[i]] not in ranking_index:\n",
    "            ranking_index.append(pareto_list[arg_index[i]])\n",
    "            ranking_MIPS.append(FT_list[arg_index[i]])\n",
    "    \n",
    "    MIPS_selected, MIPS_scores = np.zeros(len(df_pareto)), np.full(len(df_pareto), fill_value=-10000.0)\n",
    "    MIPS_selected[ranking_index] += 1\n",
    "    MIPS_scores[ranking_index] = ranking_MIPS\n",
    "    df_pareto[\"MIPS_selected\"] = MIPS_selected\n",
    "    df_pareto[\"MIPS_score\"] = MIPS_scores\n",
    "    return df_pareto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieving generated sequences  \n",
    "Retrieving generated sequences during LaMBO trainings.  \n",
    "For each corresponding run, the table of generated sequences should be uploaded to your wandb workspace as \"task_mugd_cnn/candidates\" or you can find them under your local wandb directory such as `./wandb/RUNTITLE/files/media/table/task_mugd_cnn/candidates_ID.table.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = {\n",
    "    \"init\":None,\n",
    "    \"CNN\":None,\n",
    "    \"BERT\":None,\n",
    "    \"DNABERT\":None,\n",
    "}\n",
    "\n",
    "initset_flg = \"initset3\"\n",
    "seed_flg = 1\n",
    "data_dir = \"./generated_seqs\"\n",
    "filelist = []\n",
    "filelist.append(\n",
    "    f\"../cache_dir/Sample2019_unmod1_{initset_flg}_numsample512_ConstructSample2019EGFP.csv\"\n",
    ")\n",
    "for tmp_file in os.listdir(data_dir):\n",
    "    if initset_flg in tmp_file and f\"seed{seed_flg}\" in tmp_file:\n",
    "        filelist.append(os.path.join(data_dir, tmp_file))\n",
    "obj_columns = [\"obj_val_0\",\"obj_val_1\",\"obj_val_2\",\"obj_val_3\"]\n",
    "obj_names = [\"AGC content [%]\", \"MRL\", \"G4 score\", \"in vitro stability\"]\n",
    "\n",
    "for tmp_file in filelist:\n",
    "    df_tmp = pd.read_csv(tmp_file)\n",
    "    df_save = pd.DataFrame([], columns=obj_names)\n",
    "    new_points = None\n",
    "    if \"Sample2019\" in tmp_file:\n",
    "        df_tmp = format_init(df_tmp)\n",
    "    for obj_column, obj_name in zip(obj_columns, obj_names):\n",
    "        tmp_obj = convert_obj_to_real(df_tmp, obj_column, obj_name)\n",
    "        if new_points is None:\n",
    "            new_points = np.expand_dims(tmp_obj, -1)\n",
    "        else:\n",
    "            new_points = np.concatenate([new_points, np.expand_dims(tmp_obj, -1)], -1)\n",
    "    df_save[\"sequence\"] = format_to_rna(np.array(df_tmp[\"cand_seq\"]))\n",
    "    df_save[obj_names] = new_points\n",
    "    if not \"Sample2019\" in tmp_file:\n",
    "        df_save[\"round_idx\"] = df_tmp[\"round_idx\"]\n",
    "    else:\n",
    "        df_save[\"round_idx\"] = [-1,]*len(df_tmp)\n",
    "    \n",
    "    if \"CNN\" in tmp_file:\n",
    "        dict_df[\"CNN\"] = df_save\n",
    "    elif \"DNABERT\" in tmp_file:\n",
    "        dict_df[\"DNABERT\"] = df_save\n",
    "    elif \"BERT\" in tmp_file:\n",
    "        dict_df[\"BERT\"] = df_save\n",
    "    else:\n",
    "        dict_df[\"init\"] = df_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ranking generated sequences\n",
    "Each cells below will rank generated sequences based on the corresponding method. The methods are either Pareto frontier, Pareto ranking, R-method, or MIPS scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing init ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CNN ...\n",
      "Processing BERT ...\n",
      "Processing DNABERT ...\n"
     ]
    }
   ],
   "source": [
    "dict_pareto = {\n",
    "    \"CNN\":None,\n",
    "    \"BERT\":None,\n",
    "    \"DNABERT\":None,\n",
    "    \"init\":None,\n",
    "}\n",
    "\n",
    "for key, df in dict_df.items():\n",
    "    print(f\"Processing {key} ...\")\n",
    "    if not key==\"init\":\n",
    "        df = pd.concat([df, dict_df[\"init\"]], axis=0)\n",
    "        all_points = df[obj_names].to_numpy()\n",
    "    else:\n",
    "        all_points = df[obj_names].to_numpy()\n",
    "    pareto_mask = pareto.is_non_dominated(torch.tensor(all_points), )\n",
    "    pareto_idx = np.argsort(all_points[pareto_mask][:,0])\n",
    "    df_tmp = df[pareto_mask.numpy()].iloc[pareto_idx,:]\n",
    "    dict_pareto[key] = df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing init ...\n",
      "Processing CNN ...\n",
      "Processing BERT ...\n",
      "Processing DNABERT ...\n"
     ]
    }
   ],
   "source": [
    "dict_pareto_ranking = {\n",
    "    \"CNN\":None,\n",
    "    \"BERT\":None,\n",
    "    \"DNABERT\":None,\n",
    "    \"init\":None,\n",
    "}\n",
    "for key, df in dict_df.items():\n",
    "    print(f\"Processing {key} ...\")\n",
    "    if not key==\"init\":\n",
    "        df = pd.concat([dict_df[\"init\"], df], axis=0)\n",
    "    df_pareto_ranking = pareto_ranking(df, df, obj_names)\n",
    "    dict_pareto_ranking[key] = df_pareto_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing init ...\n",
      "Processing CNN ...\n",
      "Processing BERT ...\n",
      "Processing DNABERT ...\n"
     ]
    }
   ],
   "source": [
    "obj_rank_values = [2, 1, 4, 3]\n",
    "dict_R_method = {\n",
    "    \"CNN\":None,\n",
    "    \"BERT\":None,\n",
    "    \"DNABERT\":None,\n",
    "    \"init\":None,\n",
    "}\n",
    "for key, df in dict_df.items():\n",
    "    print(f\"Processing {key} ...\")\n",
    "    if not key==\"init\":\n",
    "        df = pd.concat([dict_df[\"init\"]], df, axis=0).reset_index(drop=True)\n",
    "    df_R_method = pareto_R_method(df, obj_names, obj_rank_values)\n",
    "    dict_R_method[key] = df_R_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing init ...\n",
      "Processing CNN ...\n",
      "Processing BERT ...\n",
      "Processing DNABERT ...\n"
     ]
    }
   ],
   "source": [
    "# pre-computing weights for calculating MIPS scores. This would take ~40min for N=4 and w=0.01.\n",
    "# weights_4 = generate_MIPS_weights(N=4, w=0.01)\n",
    "# np.save(weights_4, \"./data/MIPS_weights_4.npy\")\n",
    "\n",
    "# This step will take ~20min.\n",
    "weights_4 = np.load(\"./data/MIPS_weights_4.npy\")\n",
    "dict_MIPS = {\n",
    "    \"CNN\":None,\n",
    "    \"BERT\":None,\n",
    "    \"DNABERT\":None,\n",
    "    \"init\":None,\n",
    "}\n",
    "for key, df in dict_df.items():\n",
    "    print(f\"Processing {key} ...\")\n",
    "    if not key==\"init\":\n",
    "        df = pd.concat([dict_df[\"init\"]], df, axis=0).reset_index(drop=True)\n",
    "    df_MIPS = pareto_MIPS(df, obj_names, weights=weights_4)\n",
    "    dict_MIPS[key] = df_MIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CNN pareto_front\n",
      "\tNumber of top-ranked sequences: 38\n",
      "Processing CNN pareto_ranking\n",
      "\tNumber of top-ranked sequences: 10\n",
      "Processing CNN pareto_R_method\n",
      "\tNumber of top-ranked sequences: 10\n",
      "Processing CNN pareto_MIPS\n",
      "\tNumber of top-ranked sequences: 10\n",
      "Processing BERT pareto_front\n",
      "\tNumber of top-ranked sequences: 95\n",
      "Processing BERT pareto_ranking\n",
      "\tNumber of top-ranked sequences: 10\n",
      "Processing BERT pareto_R_method\n",
      "\tNumber of top-ranked sequences: 10\n",
      "Processing BERT pareto_MIPS\n",
      "\tNumber of top-ranked sequences: 10\n",
      "Processing DNABERT pareto_front\n",
      "\tNumber of top-ranked sequences: 49\n",
      "Processing DNABERT pareto_ranking\n",
      "\tNumber of top-ranked sequences: 10\n",
      "Processing DNABERT pareto_R_method\n",
      "\tNumber of top-ranked sequences: 10\n",
      "Processing DNABERT pareto_MIPS\n",
      "\tNumber of top-ranked sequences: 10\n"
     ]
    }
   ],
   "source": [
    "dict_colors = {\"CNN\":np.array([79, 173, 234])/255,\n",
    "             \"BERT\":np.array([239, 206, 69])/255,\n",
    "             \"DNABERT\":np.array([127, 23, 14])/255}\n",
    "pareto_ranking_top_N = 10\n",
    "SAVEFIG=True\n",
    "\n",
    "for tmp_model in [\"CNN\", \"BERT\", \"DNABERT\"]:\n",
    "    for pareto_or_ranking in [\"pareto_front\", \"pareto_ranking\", \"pareto_R_method\", \"pareto_MIPS\"]:\n",
    "        print(f\"Processing {tmp_model} {pareto_or_ranking}\")\n",
    "        tmp_color = dict_colors[tmp_model]\n",
    "\n",
    "        fig = plt.figure(figsize=(16,8),\n",
    "                            facecolor=[0,0,0,0])\n",
    "        ax = fig.subplots(nrows=1, ncols=1)\n",
    "\n",
    "        obj_names = [\"AGC content [%]\", \"MRL\", \"G4 score\", \"in vitro stability\"]\n",
    "        plt.rcParams[\"axes.grid\"] = False\n",
    "        ax.set_facecolor([1,1,1])\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        x_offset = 0.05\n",
    "        obj_labels = [\"%AGC\", \"MRL\", \"G4\", \"Stablity\"]\n",
    "        ax.set_xlim([0, 1])\n",
    "        x_vis = np.linspace(x_offset, 1-x_offset, len(obj_labels))\n",
    "        ax.set_xticks(x_vis)\n",
    "        ax.set_xticklabels(obj_labels, fontsize=18)\n",
    "        ax.tick_params(axis=\"x\", which=\"major\", pad=10)\n",
    "\n",
    "        y_mins = np.array([60, 0.0, -1.0, -0.44])\n",
    "        y_maxs = np.array([100, 9.0, 0, -0.41])\n",
    "        x_distance = (1 - 2*x_offset)/(len(obj_names)-1)\n",
    "        y_ranges = y_maxs - y_mins\n",
    "        y_pad = 0.02\n",
    "        y_mins_padded = y_mins - y_ranges * y_pad\n",
    "        y_maxs_padded = y_maxs + y_ranges * y_pad\n",
    "        for i, obj_name in enumerate(obj_names):\n",
    "            ymin = y_mins_padded[i]\n",
    "            ymax = y_maxs_padded[i]\n",
    "            new_twin = ax.twinx()\n",
    "            new_twin.spines.right.set_position((\"axes\", x_offset + x_distance*i))\n",
    "            new_twin.set_ylim([ymin, ymax])\n",
    "            new_twin.spines[\"right\"].set_color(\"black\")\n",
    "            new_twin.tick_params(direction=\"inout\", length=10, labelsize=14)\n",
    "            new_twin.tick_params(axis=\"y\", pad=5)\n",
    "            new_twin.spines[\"left\"].set_visible(False)\n",
    "            if i==0:\n",
    "                y_ticks = np.arange(y_mins[i], y_maxs[i]+1, 10)\n",
    "                new_twin.set_yticks(y_ticks, labels=[str(y_mins[i])] + [\"\"]*(len(y_ticks)-2) + [str(y_maxs[i])])\n",
    "            elif i==1:\n",
    "                y_ticks = np.arange(y_mins[i], y_maxs[i]+0.1, 1.0)\n",
    "                new_twin.set_yticks(y_ticks, labels=[str(y_mins[i])] + [\"\"]*(len(y_ticks)-2) + [str(y_maxs[i])])\n",
    "            elif i==2:\n",
    "                y_ticks = np.arange(y_mins[i], y_maxs[i]+0.01, 0.1)\n",
    "                new_twin.set_yticks(y_ticks, labels=[\"$\\minus$1.0\",] + [\"\"]*(len(y_ticks)-2) + [\"0.0\"])\n",
    "            elif i==3:\n",
    "                y_ticks = np.arange(y_mins[i], y_maxs[i]+0.001, 0.01)\n",
    "                new_twin.set_yticks(y_ticks, labels=[\"$\\minus$0.44\",] + [\"\"]*(len(y_ticks)-2) + [\"$\\minus$0.41\"])\n",
    "\n",
    "        y_ranges_padded = y_maxs_padded - y_mins_padded\n",
    "        df_vis = dict_df[\"init\"].drop_duplicates(\"sequence\")\n",
    "        for i in range(len(df_vis)):\n",
    "            y_vis = df_vis[obj_names].iloc[i]\n",
    "            y_vis = (y_vis - y_mins) / y_ranges\n",
    "            ax.plot(x_vis, y_vis, linestyle=\"-\", color=\"gray\", alpha=0.1)\n",
    "\n",
    "        ax.set_ylim([-y_pad, 1+y_pad])\n",
    "\n",
    "        if pareto_or_ranking == \"pareto_front\":\n",
    "            df_vis = dict_pareto[tmp_model].drop_duplicates(\"sequence\")\n",
    "        elif pareto_or_ranking == \"pareto_ranking\":\n",
    "            df_vis = dict_pareto_ranking[tmp_model].drop_duplicates(\"sequence\").iloc[:pareto_ranking_top_N]\n",
    "        elif pareto_or_ranking == \"pareto_R_method\":\n",
    "            df_vis = dict_R_method[tmp_model].drop_duplicates(\"sequence\").iloc[:pareto_ranking_top_N]\n",
    "        elif pareto_or_ranking == \"pareto_MIPS\":\n",
    "            df_vis = dict_MIPS[tmp_model].drop_duplicates(\"sequence\")\n",
    "            df_vis = df_vis.sort_values(by=\"MIPS_score\", ascending=False).iloc[:pareto_ranking_top_N]\n",
    "\n",
    "        print(f\"\\tNumber of top-ranked sequences: {len(df_vis)}\")\n",
    "        for i in range(len(df_vis)):\n",
    "            y_vis = df_vis[obj_names].iloc[i]\n",
    "            y_vis = (y_vis - y_mins) / y_ranges\n",
    "            ax.plot(x_vis, y_vis, linestyle=\"-\", color=tmp_color, alpha=0.6, linewidth=2)\n",
    "\n",
    "        if pareto_or_ranking == \"pareto_front\":\n",
    "            ax.set_title(f\"{tmp_model} Pareto Front\", fontsize=20, pad=10)\n",
    "        elif pareto_or_ranking == \"pareto_ranking\":\n",
    "            ax.set_title(f\"{tmp_model} Pareto Ranking Top{pareto_ranking_top_N}\", fontsize=20, pad=10)\n",
    "        elif pareto_or_ranking == \"pareto_R_method\":\n",
    "            ax.set_title(f\"{tmp_model} R Method Top{pareto_ranking_top_N}\", fontsize=20, pad=10)\n",
    "        elif pareto_or_ranking == \"pareto_MIPS\":\n",
    "            ax.set_title(f\"{tmp_model} MIPS Top{pareto_ranking_top_N}\", fontsize=20, pad=10)\n",
    "        fig.tight_layout()\n",
    "        if SAVEFIG:\n",
    "            fig.savefig(f\"./figs/{pareto_or_ranking}_ppd_{tmp_model}.png\", dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
